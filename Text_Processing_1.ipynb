{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Processing - 1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/surajrs777/NLP/blob/main/Text_Processing_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G1qogZ6YHeaV"
      },
      "source": [
        "**Text Processing - 1**\n",
        "1.\tTokenize Text using NLTK\n",
        "2.\tSentence Tokenization\n",
        "3.\tPunktSentenceTokenizer \n",
        "4.\tTokenize sentence of different language\n",
        "5.\tWord Tokenization \n",
        "6.\tUsing TreebankWordTokenizer\n",
        "7.\tPunktWordTokenizer \n",
        "8.\tWordPunctTokenizer \n",
        "9.\tUsing Regular Expression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69ifY18UHiSf"
      },
      "source": [
        "**1. Tokenize Text using NLTK**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8XYXmxgjHGSM"
      },
      "source": [
        "# import the existing word and sentence tokenizing \n",
        "# libraries\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "  \n",
        "text = \"Natural language processing (NLP) is a field \" + \\\n",
        "       \"of computer science, artificial intelligence \" + \\\n",
        "       \"and computational linguistics concerned with \" + \\\n",
        "       \"the interactions between computers and human \" + \\\n",
        "       \"(natural) languages, and, in particular, \" + \\\n",
        "       \"concerned with programming computers to \" + \\\n",
        "       \"fruitfully process large natural language \" + \\\n",
        "       \"corpora. Challenges in natural language \" + \\\n",
        "       \"processing frequently involve natural \" + \\\n",
        "       \"language understanding, natural language\" + \\\n",
        "       \"generation frequently from formal, machine\" + \\\n",
        "       \"-readable logical forms), connecting language \" + \\\n",
        "       \"and machine perception, managing human-\" + \\\n",
        "       \"computer dialog systems, or some combination \" + \\\n",
        "       \"thereof.\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPidKoqUIaOK"
      },
      "source": [
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQmSzM06IdmC",
        "outputId": "aa4566a4-ca93-4c2a-bfb0-d4272594cb0c"
      },
      "source": [
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlgfIco8IXPd",
        "outputId": "2aba1f72-5ace-4670-b5f0-0648139ad3df"
      },
      "source": [
        "print(sent_tokenize(text))\n",
        "print(word_tokenize(text))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.', 'Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.']\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', ',', 'machine-readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human-computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX9OGoJcIs5S"
      },
      "source": [
        "**2. Sentence Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pd4yWIw1IwIs",
        "outputId": "cebc13e3-6141-4867-dd8c-1c96b03e1776"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks. You are studying NLP article\"\n",
        "sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o5AQypykJAf7"
      },
      "source": [
        "**3. PunktSentenceTokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z8bAA2OdJEdd",
        "outputId": "a5b26975-12d8-4025-bb0c-d31f3b043f51"
      },
      "source": [
        "import nltk.data\n",
        "  \n",
        "# Loading PunktSentenceTokenizer using English pickle file\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/PY3/english.pickle')\n",
        "  \n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello everyone.',\n",
              " 'Welcome to GeeksforGeeks.',\n",
              " 'You are studying NLP article']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhZbH2LcJZ6K"
      },
      "source": [
        "**4. Tokenize sentence of different language**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8oYgMdoJdyL",
        "outputId": "8baf2a65-59c3-4722-ddb6-1f635fef042c"
      },
      "source": [
        "import nltk.data\n",
        "  \n",
        "spanish_tokenizer = nltk.data.load('tokenizers/punkt/PY3/spanish.pickle')\n",
        "  \n",
        "text = 'Hola amigo. Estoy bien.'\n",
        "spanish_tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hola amigo.', 'Estoy bien.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w2votYi5JjPa"
      },
      "source": [
        "**5. Word Tokenization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vv7E2zKEJ2dL",
        "outputId": "aec1911e-44fe-4430-850d-b63b31189eff"
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "  \n",
        "text = \"Hello everyone. Welcome to GeeksforGeeks.\"\n",
        "word_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone', '.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EO-osoOaJ49y"
      },
      "source": [
        "**6. Using TreebankWordTokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9iux2rhKBJq",
        "outputId": "f95ea79d-ecf2-4174-a991-7774fba3a9c3"
      },
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "  \n",
        "tokenizer = TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello', 'everyone.', 'Welcome', 'to', 'GeeksforGeeks', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yP2ODNfxKDUr"
      },
      "source": [
        "**7 . PunktWordTokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "HITH5TL-OYMG",
        "outputId": "5c43bcb9-a83f-46ad-c32d-d834e725e691"
      },
      "source": [
        "#PunktWordTokenizer – It doen’t seperates the punctuation from the words.\n",
        "from nltk.tokenize import PunktWordTokenizer\n",
        "  \n",
        "tokenizer = PunktWordTokenizer()\n",
        "tokenizer.tokenize(\"Let's see how it's working.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-83f41c678104>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#PunktWordTokenizer – It doen’t seperates the punctuation from the words.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPunktWordTokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPunktWordTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Let's see how it's working.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'PunktWordTokenizer' from 'nltk.tokenize' (/usr/local/lib/python3.7/dist-packages/nltk/tokenize/__init__.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q2MDtH1iUbRS"
      },
      "source": [
        "**8. WordPunctTokenizer**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hxEWNEwbUhoJ",
        "outputId": "a46d1fde-a8a9-42ae-b3a4-151ddb1f418b"
      },
      "source": [
        "#WordPunctTokenizer – It seperates the punctuation from the words.\n",
        "\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "  \n",
        "tokenizer = WordPunctTokenizer()\n",
        "tokenizer.tokenize(\"Let's see how it's working.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Let', \"'\", 's', 'see', 'how', 'it', \"'\", 's', 'working', '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VhsqE1pfUzD6"
      },
      "source": [
        "**9. Using Regular Expression**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iq7wJqokU4zs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e790bb2d-e0b6-4ec1-ffa1-f33fff3728a3"
      },
      "source": [
        "from nltk.tokenize import regexp_tokenize\n",
        "  \n",
        "text = \"Let's see how it's working.\"\n",
        "regexp_tokenize(text, \"[\\w']+\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"Let's\", 'see', 'how', \"it's\", 'working']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    }
  ]
}